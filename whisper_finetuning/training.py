# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/training.ipynb.

# %% auto 0
__all__ = ['freeze_encoder', 'get_optimizer', 'WhisperTrainer']

# %% ../nbs/training.ipynb 1
from transformers.trainer_pt_utils import get_parameter_names
from tqdm import tqdm

import torch.optim as optim
import torch.nn as nn
import torch
import wandb
import os
import gc
import re

import numpy as np

from .metrics import WER

# %% ../nbs/training.ipynb 2
def freeze_encoder(model):
    for n, p in model.named_parameters():
        if "encoder" in n:
            p.requires_grad = False

# %% ../nbs/training.ipynb 3
def get_optimizer(optimizer_name, model, learning_rate, weight_decay):
    decay_parameters = get_parameter_names(model, [nn.LayerNorm])
    decay_parameters = [name for name in decay_parameters if "bias" not in name]

    optimizer_grouped_parameters = [
        {"params": [p for n, p in model.named_parameters() if n in decay_parameters],
         "weight_decay": weight_decay},
        {"params": [p for n, p in model.named_parameters() if n not in decay_parameters],
         "weight_decay": 0.0}]

    if optimizer_name == "adam":
        return optim.Adam(optimizer_grouped_parameters, lr=learning_rate)
    if optimizer_name == "adamw":
        return optim.AdamW(optimizer_grouped_parameters, lr=learning_rate)

    raise ValueError(f"Unknown optimizer name: {optimizer_name}")

# %% ../nbs/training.ipynb 4
class WhisperTrainer:
    def __init__(self, model, processor, optimizer, cfg, lr_scheduler=None):
        
        self.cfg = cfg
        
        self.model = model
        self.processor = processor

        self.fp16 = cfg.fp16
        self.device = cfg.device
        self.save_path = os.path.join(cfg.save_path, cfg.config_name)
        self.grad_accum_steps = cfg.grad_accum_steps
        self.grad_clip_norm = cfg.grad_clip_norm
        
        self.first_eval_epoch = cfg.first_eval_epoch
        
        self.optimizer = optimizer
        self.lr_scheduler = lr_scheduler
        self.scaler = torch.cuda.amp.GradScaler(enabled=self.fp16)
        
        self.validation_step = cfg.validation_step
        self.wandb_log = cfg.wandb_log
        
        self.metrics_dict = cfg.metrics_dict
        self.best_metrics = {metric: np.inf for metric in self.metrics_dict}

        if self.save_path is not None:
            os.makedirs(self.save_path, exist_ok=True)

    def get_postfix(self, loss, stage, log_metrics=False):
        
        postfix = {
            **({f"{stage} loss": f"{(loss):.3f}"} if stage == "train" else {}),
            **({metric_name: f"{metric.get_metric():.3f}" for metric_name, metric \
                in self.metrics_dict.items()} if (stage == "valid" and log_metrics) else {})}

        return postfix

    def log_to_wandb(self, step, loss, stage):
        metrics_to_log = {f"{stage}/step": step}
        
        if stage == "valid":
            for metric_name, metric in self.metrics_dict.items():
                metrics_to_log[f"{stage}/{metric_name}"] = metric.get_metric()
            
        if stage == "train" and self.lr_scheduler is not None:
            metrics_to_log[f"{stage}/{stage}_loss"] = loss
            metrics_to_log[f"{stage}/{stage}_lr"] = self.lr_scheduler.get_last_lr()[0]
        
        wandb.log(metrics_to_log)

    def compute_validation_metrics(self):
        for metric_name in self.metrics_dict:
            self.metrics_dict[metric_name].compute()

    def reset_metrics(self):
        for _, metric in self.metrics_dict.items():
            metric.reset()
        
    def process_batch(self, batch, stage="train"):
        labels = batch["labels"]
        input_features = batch["input_features"].to(self.device)
        decoder_attention_mask = batch["decoder_attention_mask"].to(self.device)
        decoder_input_ids = batch["decoder_input_ids"].to(self.device)
        target_texts = batch["target_text"]

        if stage == "train":
            labels = batch["labels"].to(self.device)

        return labels, decoder_attention_mask, input_features, \
            decoder_input_ids, target_texts

    def train_one_step(self, step, batch, total_steps):
        
        labels, decoder_attention_masks, input_features, _, _ = self.process_batch(batch)

        with torch.cuda.amp.autocast(enabled=self.fp16):
            outputs = self.model(input_features=input_features, labels=labels, decoder_attention_mask=decoder_attention_masks)

        batch_loss = outputs["loss"] / self.grad_accum_steps
        self.scaler.scale(batch_loss).backward()
        
        if (step % self.grad_accum_steps == 0) or (step == total_steps):
            self.scaler.unscale_(self.optimizer)
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip_norm)

            self.scaler.step(self.optimizer)
            scale = self.scaler.get_scale()
            self.scaler.update()

            if self.lr_scheduler is not None and (scale <= self.scaler.get_scale()):
                self.lr_scheduler.step()
                
            self.optimizer.zero_grad(set_to_none=True)
        
        return batch_loss.item()

    def train_one_epoch(self, epoch, train_dataloader, valid_dataloader):
        
        self.model.train()

        total_steps = len(train_dataloader) * (epoch)
        start_step = (epoch - 1) * len(train_dataloader) + 1
        total_train_loss = 0

        with tqdm(enumerate(train_dataloader, start_step), unit="batch", total=total_steps, bar_format='{l_bar}{bar:10}{r_bar}', position=0, leave=True, initial=start_step) as progress_bar:

            progress_bar.set_description(f"Epoch {epoch}")

            for step, batch in progress_bar:
                    
                internal_step = (step + 1) - start_step
                
                total_train_loss += self.train_one_step(step, batch, total_steps=total_steps)
                self.checkpoint(step, total_train_loss, "train")
                
                if (internal_step % self.grad_accum_steps == 0) or (step == total_steps):
                    current_step = step // self.grad_accum_steps
                    current_loss = total_train_loss / internal_step
                    
                    progress_bar.set_postfix(self.get_postfix(current_loss, "train"))
                    
                    if self.wandb_log:
                        self.log_to_wandb(current_step, current_loss, "train")
                        
                if epoch >= self.first_eval_epoch and (step % self.validation_step) == 0 and (step != 0):
                    progress_bar.set_postfix("")
                    self.validate_one_epoch(step, valid_dataloader)
                    self.model.train()

                
        total_train_loss /= total_steps
        
        torch.cuda.empty_cache()
        gc.collect()
        
        return total_train_loss

    def validate_one_step(self, batch):
        _, _, input_features, \
            decoder_input_ids, target_texts = self.process_batch(batch)

        with torch.cuda.amp.autocast(enabled=self.fp16):

            predictions = self.model.generate(input_features=input_features, \
                decoder_input_ids=decoder_input_ids, max_new_tokens=256)
           
            predictions = self.processor.batch_decode(predictions, skip_special_tokens=True)
            predictions = [re.sub(r" +", " ", prediction).strip() for prediction in predictions]

            for metric_name in self.metrics_dict:
                self.metrics_dict[metric_name].update(predictions, target_texts)

    @torch.no_grad()
    def validate_one_epoch(self, global_step, dataloader):
        
        self.model.eval()
        
        total_valid_loss = 0
        total_steps = len(dataloader)

        with tqdm(enumerate(dataloader, 1), unit="batch", bar_format='{l_bar}{bar:10}{r_bar}', total=total_steps, position=0, leave=True) as progress_bar:
            progress_bar.set_description(f"Validation {global_step}".ljust(15))

            for step, batch in progress_bar:
                self.validate_one_step(batch)

                if step == total_steps:
                    self.compute_validation_metrics()

                progress_bar.set_postfix(
                    self.get_postfix(None, "valid", log_metrics=(step==total_steps)))

        total_valid_loss /= total_steps

        if self.wandb_log:
            self.log_to_wandb(global_step, total_valid_loss, "valid")
            
        self.checkpoint(global_step, total_valid_loss, "valid")
        self.reset_metrics()

        torch.cuda.empty_cache()
        gc.collect()
        
    def checkpoint(self, global_step, loss, stage):
        if self.save_path is None:
            return

        if global_step % 100 == 0 and stage == "train":

            self.model.save_pretrained(os.path.join(self.save_path, "last"))

            torch.save({
                'loss': loss,
                'global_step': global_step,
                'scaler_state_dict': self.scaler.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'lr_scheduler_state_dict': self.lr_scheduler.state_dict(),
                }, os.path.join(self.save_path, "last", "training_states.pth"))

            return

        if stage == "train":
            return
        
        for metric_name, best_metric_value in self.best_metrics.items():
            matric_value = self.metrics_dict[metric_name].get_metric()
            
            if matric_value > best_metric_value:
                continue

            self.best_metrics[metric_name] = matric_value
            self.model.save_pretrained(os.path.join(self.save_path, f"best_{metric_name}"))

    def train(self, epochs, train_dataloader, valid_dataloader):
        
        torch.cuda.empty_cache()
            
        for epoch in range(1, epochs+1):
            self.train_one_epoch(epoch, train_dataloader, valid_dataloader)

        self.validate_one_epoch(len(train_dataloader) * epochs, valid_dataloader)
        
        return self.best_metrics
