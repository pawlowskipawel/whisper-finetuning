# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/data.ipynb.

# %% auto 0
__all__ = ['DataCollatorCTCWithPadding', 'WhisperDataset']

# %% ../nbs/data.ipynb 1
from typing import Dict, List, Optional, Union
from transformers import WhisperProcessor
from torch.utils.data import Dataset
from dataclasses import dataclass

import torch
import os

import pandas as pd
import numpy as np

# %% ../nbs/data.ipynb 2
@dataclass
class DataCollatorCTCWithPadding:

    processor: WhisperProcessor
    training: bool = True
    padding: Union[bool, str] = True
    max_length: Optional[int] = None
    max_length_labels: Optional[int] = None
    pad_to_multiple_of: Optional[int] = None
    pad_to_multiple_of_labels: Optional[int] = None

    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        batch = {"input_features": torch.tensor(np.array([feature["input_features"] for feature in features]), dtype=torch.float)}
        
        if self.training:
            labels = [{"input_ids": feature["labels"]} for feature in features]
            labels_padded = self.processor.tokenizer.pad(
                labels,
                padding=self.padding,
                max_length=self.max_length_labels,
                pad_to_multiple_of=self.pad_to_multiple_of_labels,
                return_tensors="pt")

            # replace padding with -100 to ignore loss correctly
            batch["labels"] = labels_padded.pop("input_ids").masked_fill(labels_padded["attention_mask"].ne(1), -100)
            batch["decoder_attention_mask"] = labels_padded.pop("attention_mask")
            batch["target_text"] = np.array([feature["target_text"] for feature in features])

        batch["decoder_input_ids"] = torch.tensor(np.stack([feature["decoder_input_ids"] for feature in features]))
        
        return batch

# %% ../nbs/data.ipynb 3
class WhisperDataset(Dataset):
    
    def __init__(self, dataset_df, processor, lang="pl", mel_dir="", mode="train_val"):
        super().__init__()
        
        assert mode in ("train_val", "inference"), "mode must be train_val or infernce"
        self.mode = mode
        
        self.mel_dir = mel_dir
        self.processor = processor
        self.dataset_df = dataset_df

        self.pretokenized = "tokenized_transcript" in self.dataset_df.columns
        
        self.decoder_prompt = f"<|startoftranscript|><|{lang}|><|transcribe|><|notimestamps|>"
        self.decoder_prompt_tokenized = self.tokenize_text(self.decoder_prompt)

    def __len__(self):
        return len(self.dataset_df)
    
    def tokenize_text(self, text):
        return np.squeeze(self.processor.tokenizer.encode(text, \
            padding="longest", return_tensors="np", add_special_tokens=False))
        
    def __getitem__(self, idx):
        filename = self.dataset_df.at[idx, "id"]
        
        mel_path = os.path.join(self.mel_dir, f"{filename}.npy")
        mel_array = np.load(mel_path, allow_pickle=True)

        output_dict = {
            "input_features": mel_array,
            "decoder_input_ids": self.decoder_prompt_tokenized}

        if self.mode == "train_val":
            
            transcript = self.dataset_df.at[idx, "transcript"]
            
            transcript_tokenized = self.dataset_df.at[idx, "tokenized_transcript"] if self.pretokenized \
                else self.tokenize_text(f"{transcript}<|endoftext|>")
            
            target_text_tokenized = np.append(self.decoder_prompt_tokenized, transcript_tokenized)

            output_dict["target_text"] = transcript
            output_dict["labels"] = target_text_tokenized

        return output_dict 
        
